# -*- coding: utf-8 -*-
"""run_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ESJlUqMBYHDqn7txCmYiR4s9xkEEjNIu
"""

#Importing standard packages required for the task.
import os
import pandas as pd
import numpy as np
import glob,csv,json

import tensorflow as tf


# If you are going to use GPU, make sure the GPU in in the output
tf.config.list_physical_devices('GPU')

#Getting Routes to all the JSON files.
def get_folder_from_directory(path_name):
  mp={}
  for _file in glob.glob(path_name):
          if _file.endswith(".json"):
              _file_name = _file.split("/")[-1].split(".")[0]
              mp[_file_name]= _file
              print(_file)
  return mp

#Function to join the Title and Text from recieved from the JSON files.
def get_text(_path):
    with open(_path) as f:
        data = json.load(f)
        data['FT'] = data['title'] + '  [SEP]  ' + data['text']
        return data['FT']

#Getting data from all the JSON files generated by the SemEval Downloader.
def text_dictonary(mp, file_name):
  cnt=0
  data_mp={} 
  with open(file_name) as f:
      rows = csv.reader(f)
      next(rows)
      for row in rows:
          article_id1,article_id2 = row[2].split("_")
          if article_id1 in mp and article_id2 in mp:
              article1_folder = mp[article_id1]
              article2_folder = mp[article_id2]
              if article_id1 not in data_mp:
                  text1=get_text(article1_folder)
                  data_mp[article_id1]=text1
              text2=get_text(article2_folder)
              if article_id2 not in data_mp:
                  text2=get_text(article2_folder)
                  data_mp[article_id2]=text2
              cnt+=1
              print(cnt,article1_folder,article2_folder)
          else:
              continue
  return data_mp

#Here a new CSV file is created with pair_ids, title + [SEP] + text, as well as all the scores for the training putpose.
def text_file(data_mp, csv_file, file):
    with open(file,"w") as lp:
        writer = csv.writer(lp)
        with open(csv_file) as fp:
            rows = csv.reader(fp)
            next(rows)
            for row in rows:
                text_1 = None
                text_2 = None
                id1,id2  = row[2].split("_")
                geo = row[7]
                enti = row[8]
                time = row[9]
                nar = row[10]
                overall = row[11]
                style = row[12]
                tone = row[13]
                if id1 in data_mp:
                    text_1 = [data_mp[id1]]
                if id2 in data_mp:
                    text_2 = [data_mp[id2]]
                else:
                  continue
                writer.writerow([row[2],text_1,text_2,geo, enti, time, nar, overall, style, tone])

train_mp = get_folder_from_directory('/content/json_dataset/*/*')
train_data_mp = text_dictonary(train_mp, "/content/semeval-2022_task8_train-data_batch.csv")
text_file(train_data_mp,"/content/semeval-2022_task8_train-data_batch.csv", 'train.csv')

labels = ['pair_id','text1', 'text2','Geography','Entities','Time','Narrative','Overall','Style','Tone']

train_data = pd.read_csv('/content/train.csv',names=labels)

print(train_data)

train_data['text1'].dropna(inplace=True)
train_data['text1'] = train_data['text1'].astype(str)
train_data['text2'].dropna(inplace=True)
train_data['text2'] = train_data['text2'].astype(str)
train_data.head()

#Preprocessing Functionality
import re


def remove_punctuation(sentence: str) -> str:
        """ Remove punctuations in sentence with re
        Args:
            sentence: sentence with possible punctuations
        Returns:
            sentence: sentence without punctuations
        """
        # Start your code here
        sentence = sentence.lower()
        sentence = re.sub(r'[^\w\s]', '', sentence)
        # End
        return sentence

def remove_url(sentence: str) -> str:
        """ Remove urls in text with re
        Args:
            sentence: sentence with possible urls
        Returns:
            sentence: sentence without urls
        """
        # Start your code here
        sentence = re.sub('http[s]://\S+|http://\S+|www.\S+','',sentence)
        # End
        return sentence

def remove_number(sentence: str) -> str:
        """ Remove numbers in sentence with re
        Args:
            sentence: sentence with possible numbers
        Returns:
            sentence: sentence without numbers
        """
        # Start your code here
        sentence = re.sub('[0-9]','',sentence)
        # End
        return sentence

train_data['preprocess_text1'] = train_data.apply(lambda x: remove_punctuation(x['text1']), axis=1)
train_data['preprocess_text1'] = train_data.apply(lambda x: remove_url(x['preprocess_text1']), axis=1)
train_data['preprocess_text1'] = train_data.apply(lambda x: remove_number(x['preprocess_text1']), axis=1)

train_data['preprocess_text2'] = train_data.apply(lambda x: remove_punctuation(x['text2']), axis=1)
train_data['preprocess_text2'] = train_data.apply(lambda x: remove_url(x['preprocess_text2']), axis=1)
train_data['preprocess_text2'] = train_data.apply(lambda x: remove_number(x['preprocess_text2']), axis=1)


train_data[['preprocess_text1','preprocess_text2']].head()

#From the SentenceTransformer, I've selected paraphrase-multilingual-mpnet-base-v2 as it has inbuilt properties for 50+ languages.
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')

#We are using this to encode the 1st and 2nd news article in all the samples. Each article is encoded into a 768 feature vector
def model_encode(file_name, data_frame, column, model):
  encoder = model.encode(data_frame.iloc[:,column],batch_size=1, show_progress_bar=True)
  encoder_series = pd.DataFrame(encoder)
  encoder_series.to_csv(file_name)
  print(encoder.shape)

model_encode("train_feature_1.csv", train_data, int(10), model)
model_encode("train_feature_2.csv", train_data, int(11), model)

#Taking the feature vectors of 1st article, 2nd article and converting them into a single feature.
def converting_feature_vectors(file_name1, file_name2):
  feature1 = pd.read_csv(file_name1).values
  feature2 = pd.read_csv(file_name2).values
  feature = feature1*feature2
  return feature

#Features and Labels for training.
train_x =  converting_feature_vectors('train_feature_1.csv','train_feature_2.csv')                            
train_y = (train_data["Overall"].values)

#Importing all the necessary tensorflow modules. 
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import Dense , Dropout
from tensorflow.keras import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import MeanAbsoluteError
from tensorflow.keras.layers import ReLU
from tensorflow.keras.callbacks import LearningRateScheduler

#Spliting training data and validation data using train_test_split.
from sklearn.model_selection import train_test_split
train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y,test_size = 0.3)

#Converting all the inputs to tensor of float32
train_x = tf.convert_to_tensor(train_x, dtype=tf.float32)
train_y = tf.convert_to_tensor(train_y, dtype=tf.float32)
valid_x = tf.convert_to_tensor(valid_x, dtype=tf.float32)
valid_y = tf.convert_to_tensor(valid_y, dtype=tf.float32)

#Declaring all the variables for the model.
input_dim = 769
hidden_dim = 600
hidden_dim_1 = 400 
hidden_dim_2 = 400 
hidden_dim_3 = 200
hidden_dim_4 = 100
hidden_dim_5 = 10
output_dim = 1
lr = 1e-5
momentum = 0.99
epsilon = 1e-3
dr_rate = 0.01

#Regression Model
def build_regression_model(input_dim, hidden_dim,  hidden_dim_1,hidden_dim_2,hidden_dim_3,hidden_dim_4,hidden_dim_5, output_dim, dr_rate):
    model = Sequential()
    model.add(Dropout(rate=dr_rate, input_shape=(input_dim,)))
    model.add(Dense(hidden_dim, input_dim=input_dim, activation='relu',  kernel_initializer='normal', kernel_regularizer='l2'))
    model.add(BatchNormalization(momentum=momentum,epsilon=epsilon))
    model.add(ReLU(max_value=4, threshold=1))
    model.add(Dense(hidden_dim_1, input_dim=hidden_dim, activation='relu', kernel_initializer='normal', kernel_regularizer='l2'))
    model.add(BatchNormalization(momentum=momentum,epsilon=epsilon))
    model.add(ReLU(max_value=4, threshold=1))
    model.add(Dense(hidden_dim_2,input_dim=hidden_dim_1, activation='relu', kernel_initializer='normal', kernel_regularizer='l2'))
    model.add(BatchNormalization(momentum=momentum,epsilon=epsilon))
    model.add(ReLU(max_value=4, threshold=1))
    model.add(Dropout(rate=dr_rate, input_shape=(hidden_dim_2,)))
    model.add(Dense(hidden_dim_3, input_dim=hidden_dim_2, activation='relu',  kernel_initializer='normal', kernel_regularizer='l2'))
    model.add(BatchNormalization(momentum=momentum,epsilon=epsilon))
    model.add(ReLU(max_value=4, threshold=1))
    model.add(Dense(hidden_dim_4,input_dim=hidden_dim_3, kernel_initializer='normal', activation='relu', kernel_regularizer='l2'))
    model.add(BatchNormalization(momentum=momentum,epsilon=epsilon))
    model.add(ReLU(max_value=4, threshold=1))
    model.add(Dense(hidden_dim_5,input_dim=hidden_dim_4, activation='relu', kernel_initializer='normal', kernel_regularizer='l2'))
    model.add(BatchNormalization(momentum=momentum,epsilon=epsilon))
    model.add(ReLU(max_value=4, threshold=1))
    model.add(Dense(output_dim,input_dim=hidden_dim_5, activation='linear',kernel_initializer='normal', kernel_regularizer='l2'))

    return model

model = build_regression_model(input_dim, hidden_dim, hidden_dim_1,hidden_dim_2,hidden_dim_3, hidden_dim_4,hidden_dim_5, output_dim, dr_rate)

#Model Summary
with open('/content/network.txt', 'w') as f:

    model.summary(print_fn=lambda x: f.write(x + '\n'))

model.summary()

#Declaring Optimizer and loss function.
optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=lr, epsilon=1e-07)
loss_func = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.AUTO)

#Early stoppiing Functionality
callback = tf.keras.callbacks.EarlyStopping(
    monitor="val_mean_absolute_error",
    min_delta=0,
    patience=5,
    verbose=1,
    mode="min",
    baseline=None,
    restore_best_weights=False,
)

#Compiling and Fitting the model.
model.compile(optimizer=optimizer, loss=loss_func, metrics=[tf.keras.metrics.MeanAbsoluteError()])
history = model.fit(train_x, train_y, batch_size=1, epochs=500,verbose=1,callbacks=[callback], validation_data=(valid_x, valid_y))

print('Model Evaluation on Train Set:')
model.evaluate(train_x,train_y)

print('Model Evaluation on Validation Set:')
model.evaluate(valid_x, valid_y)

from matplotlib import pyplot as plt

#Using matplotlib to plot the training loss and validation loss.
plt.plot(np.array(history.history['loss']), label="train")
plt.plot(np.array(history.history['val_loss']), label="validation")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training with Dropout: 0.01 Epoch: 500, learning Rate:1e-5, momentum:0.99 ,epsilon:1e-3, Adam optimizer")
plt.legend()
plt.show()

#Predicting the outputs from the model for the train and validation set.
y_pred_train = model.predict(train_x, verbose='auto')
y_pred_valid = model.predict(valid_x, verbose='auto')

y_pred_train = np.array(y_pred_train)
y_pred_valid = np.array(y_pred_valid)

#Bounding the predictions from 1 to 4.
for i in range(y_pred_train.shape[0]):
  if y_pred_train[i] < 1:
    y_pred_train[i] = 1
  if y_pred_train[i] > 4:
    y_pred_train[i] = 4
  else:
    continue
for i in range(y_pred_valid.shape[0]):
  if y_pred_valid[i] < 1:
    y_pred_valid[i] == 1
  if y_pred_valid[i] > 4:
    y_pred_valid[i] == 4
  else:
    continue

#Using the Mean Absolute Error Evaluation Metric from sklearn.
from sklearn.metrics import mean_absolute_error

train_score = mean_absolute_error(train_y, y_pred_train)
valid_score = mean_absolute_error(valid_y, y_pred_valid)

#The MAE scores the training and validation set.
print('sklearn Training MAE Score:', train_score)
print('sklearn Validation MAE Score:', valid_score)

#Saving the model for Testing.
model.save('/content/test_model.h5')