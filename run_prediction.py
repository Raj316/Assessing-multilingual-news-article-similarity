# -*- coding: utf-8 -*-
"""run_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n-z0In-kv_jzcGhtRWlaR_wnUU6av45c
"""

#Importing standard packages required for the task.
import os
import pandas as pd
import numpy as np
import glob,csv,json

#Checking to see if GPU is available or not.
import tensorflow as tf


# If you are going to use GPU, make sure the GPU in in the output
tf.config.list_physical_devices('GPU')

#Getting Routes to all the JSON files.
def get_folder_from_directory(path_name):
  mp={}
  for _file in glob.glob(path_name):
          if _file.endswith(".json"):
              _file_name = _file.split("/")[-1].split(".")[0]
              mp[_file_name]= _file
              print(_file)
  return mp

#Function to join the Title and Text from recieved from the JSON files.
def get_text(_path):
    with open(_path) as f:
        data = json.load(f)
        data['FT'] = data['title'] + '  [SEP]  ' + data['text']
        return data['FT']

#Getting data from all the JSON files generated by the SemEval Downloader.
def text_dictonary(mp, file_name):
  cnt=0
  data_mp={} 
  with open(file_name) as f:
      rows = csv.reader(f)
      next(rows)
      for row in rows:
          article_id1,article_id2 = row[2].split("_")
          if article_id1 in mp and article_id2 in mp:
              article1_folder = mp[article_id1]
              article2_folder = mp[article_id2]
              if article_id1 not in data_mp:
                  text1=get_text(article1_folder)
                  data_mp[article_id1]=text1
              text2=get_text(article2_folder)
              if article_id2 not in data_mp:
                  text2=get_text(article2_folder)
                  data_mp[article_id2]=text2
              cnt+=1
              print(cnt,article1_folder,article2_folder)
          else:
              continue
  return data_mp

#Here a new CSV file is created with pair_ids, title + [SEP] + text, as well as all the scores for the training putpose.
def text_file(data_mp, csv_file, file):
    with open(file,"w") as lp:
        writer = csv.writer(lp)
        with open(csv_file) as fp:
            rows = csv.reader(fp)
            next(rows)
            for row in rows:
                text_1 = None
                text_2 = None
                id1,id2  = row[2].split("_")
                geo = row[7]
                enti = row[8]
                time = row[9]
                nar = row[10]
                overall = row[11]
                style = row[12]
                tone = row[13]
                if id1 in data_mp:
                    text_1 = [data_mp[id1]]
                if id2 in data_mp:
                    text_2 = [data_mp[id2]]
                else:
                  continue
                writer.writerow([row[2],text_1,text_2,geo, enti, time, nar, overall, style, tone])

test_mp = get_folder_from_directory('/content/test_data/*/*')
test_data_mp = text_dictonary(test_mp, "/content/final_evaluation_data.csv")
text_file(test_data_mp,"/content/final_evaluation_data.csv", 'test.csv')

labels = ['pair_id','text1', 'text2','Geography','Entities','Time','Narrative','Overall','Style','Tone']

test_data = pd.read_csv('/content/test.csv',names=labels)

print(test_data)

test_data['text1'].dropna(inplace=True)
test_data['text1'] = test_data['text1'].astype(str)
test_data['text2'].dropna(inplace=True)
test_data['text2'] = test_data['text2'].astype(str)
test_data.head()

#Preprocessing Functionality
import re


def remove_punctuation(sentence: str) -> str:
        """ Remove punctuations in sentence with re
        Args:
            sentence: sentence with possible punctuations
        Returns:
            sentence: sentence without punctuations
        """
        # Start your code here
        sentence = sentence.lower()
        sentence = re.sub(r'[^\w\s]', '', sentence)
        # End
        return sentence

def remove_url(sentence: str) -> str:
        """ Remove urls in text with re
        Args:
            sentence: sentence with possible urls
        Returns:
            sentence: sentence without urls
        """
        # Start your code here
        sentence = re.sub('http[s]://\S+|http://\S+|www.\S+','',sentence)
        # End
        return sentence

def remove_number(sentence: str) -> str:
        """ Remove numbers in sentence with re
        Args:
            sentence: sentence with possible numbers
        Returns:
            sentence: sentence without numbers
        """
        # Start your code here
        sentence = re.sub('[0-9]','',sentence)
        # End
        return sentence

test_data['preprocess_text1'] = test_data.apply(lambda x: remove_punctuation(x['text1']), axis=1)
test_data['preprocess_text1'] = test_data.apply(lambda x: remove_url(x['preprocess_text1']), axis=1)
test_data['preprocess_text1'] = test_data.apply(lambda x: remove_number(x['preprocess_text1']), axis=1)

test_data['preprocess_text2'] = test_data.apply(lambda x: remove_punctuation(x['text2']), axis=1)
test_data['preprocess_text2'] = test_data.apply(lambda x: remove_url(x['preprocess_text2']), axis=1)
test_data['preprocess_text2'] = test_data.apply(lambda x: remove_number(x['preprocess_text2']), axis=1)


test_data[['preprocess_text1','preprocess_text2']].head()

#From the SentenceTransformer, I've selected paraphrase-multilingual-mpnet-base-v2 as it has inbuilt properties for 50+ languages.
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')

#We are using this to encode the 1st and 2nd news article in all the samples. Each article is encoded into a 768 feature vector
def model_encode(file_name, data_frame, column, model):
  encoder = model.encode(data_frame.iloc[:,column],batch_size=1, show_progress_bar=True)
  encoder_series = pd.DataFrame(encoder)
  encoder_series.to_csv(file_name)
  print(encoder.shape)

model_encode("test_feature_1.csv", test_data, int(10), model)
model_encode("test_feature_2.csv", test_data, int(11), model)

#Taking the feature vectors of 1st article, 2nd article and converting them into a single feature.
def converting_feature_vectors(file_name1, file_name2):
  feature1 = pd.read_csv(file_name1).values
  feature2 = pd.read_csv(file_name2).values
  feature = feature1*feature2
  return feature

#Features and Labels for training.
test_x =  converting_feature_vectors('test_feature_1.csv','test_feature_2.csv')                            
test_y = (test_data["Overall"].values)

#Importing tensorflow to load the pretrained model. 
import tensorflow as tf

#Converting all the inputs to tensor of float32
test_x = tf.convert_to_tensor(test_x, dtype=tf.float32)
test_y = tf.convert_to_tensor(test_y, dtype=tf.float32)

#Loading the saved training model.
new_model = tf.keras.models.load_model('/content/test_model.h5')

new_model.summary()

#Predicting values on the test set.
y_pred = new_model.predict(test_x)

print('Model Evaluation on Test Set:')
new_model.evaluate(test_x, test_y)

y_pred = np.array(y_pred)
test_y = np.array(test_y)

#Bounding the prediction on scale of 1 to 4.
for i in range(y_pred.shape[0]):
  if y_pred[i] < 1:
    y_pred[i] = 1
  if y_pred[i] > 4:
    y_pred[i] = 4
  else:
    continue

#Importing Mean Absolute Error from sklearn.metrics to evaluate the test set.
from sklearn.metrics import mean_absolute_error

test_score = mean_absolute_error(test_y, y_pred)

#Test Score
print('sklearn Test MAE score:',test_score)